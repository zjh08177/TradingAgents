#!/usr/bin/env python3
"""
Ultra-Fast News Analyst Implementation
Bypasses LLM calls for direct news data fetching and analysis.
Similar to market_analyst_ultra_fast.py and fundamentals_analyst_ultra_fast.py.
"""

import time
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

# Import verification logging
from ..utils.verification_logging import global_tracker, verify_analyst_completion

# Import Universal Validator for comprehensive monitoring
from ..monitoring.universal_validator import validate, ValidationSeverity

logger = logging.getLogger(__name__)

def create_news_analyst_ultra_fast(llm, toolkit):
    """Create ultra-fast news analyst node (bypasses LLM for direct API calls)"""
    
    async def news_analyst_ultra_fast_node(state: Dict[str, Any]) -> Dict[str, Any]:
        """Ultra-fast news analyst with direct API calls"""
        analyst_name = "news"
        start_time = time.time()
        
        logger.info(f"ðŸ“° NEWS_ANALYST_ULTRA_FAST: Starting analysis")
        
        try:
            # Extract state parameters
            company = state.get("company_of_interest", "UNKNOWN")
            current_date = state.get("trade_date", "")
            
            logger.info(f"ðŸ“° NEWS_ANALYST_ULTRA_FAST: Analyzing {company} on {current_date}")
            
            # ðŸ” UNIVERSAL VALIDATION: Tool call start validation for news gathering
            news_tool_validation = validate("tool_call_start", 
                                           tool_name="gather_news_data", 
                                           tool_args={"company": company, "current_date": current_date}, 
                                           context=f"news_data_gathering_{company}")
            if news_tool_validation.severity in [ValidationSeverity.ERROR, ValidationSeverity.CRITICAL]:
                logger.error(f"ðŸš¨ NEWS TOOL CALL START VALIDATION FAILED: {news_tool_validation.message}")
            
            # Phase 1: Gather news data from multiple sources
            news_data_start = time.time()
            news_data = await gather_news_data(company, toolkit, start_time)
            news_data_time = time.time() - news_data_start
            
            # ðŸ” UNIVERSAL VALIDATION: Tool call response validation for news data
            news_response_validation = validate("tool_call_response",
                                              tool_name="gather_news_data",
                                              response=news_data,
                                              execution_time=news_data_time,
                                              context=f"news_gather_{company}")
            if news_response_validation.severity in [ValidationSeverity.ERROR, ValidationSeverity.CRITICAL]:
                logger.error(f"ðŸš¨ NEWS RESPONSE VALIDATION FAILED: {news_response_validation.message}")
            
            # ðŸ” UNIVERSAL VALIDATION: Data completeness validation for news data
            if isinstance(news_data, dict):
                data_completeness_validation = validate("api_response",
                                                       response=news_data,
                                                       expected_schema={"serper_articles": list, "finnhub_news": list},
                                                       context=f"news_data_structure_{company}")
                if data_completeness_validation.severity in [ValidationSeverity.ERROR, ValidationSeverity.CRITICAL]:
                    logger.error(f"ðŸš¨ NEWS DATA STRUCTURE VALIDATION FAILED: {data_completeness_validation.message}")
            
            # Phase 2: Analyze and generate structured report
            report = generate_news_report(company, news_data, current_date)
            
            # Phase 3: Log completion
            execution_time = time.time() - start_time
            verify_analyst_completion(analyst_name, "completed", execution_time, report)
            
            logger.info(f"ðŸ“° NEWS_ANALYST_ULTRA_FAST: Completed in {execution_time:.3f}s")
            logger.info(f"ðŸ“Š NEWS_ANALYST_ULTRA_FAST: Report length: {len(report)} chars")
            
            # Prepare new state for validation
            new_state = {
                "news_report": report,
                "news_messages": [],  # No LLM messages needed
                "sender": "News Analyst (Ultra-Fast)"
            }
            
            # ðŸ” UNIVERSAL VALIDATION: State transition validation
            state_validation = validate("state_transition",
                                      old_state=state,
                                      new_state={**state, **new_state},
                                      transition="news_analysis_complete")
            if state_validation.severity in [ValidationSeverity.ERROR, ValidationSeverity.CRITICAL]:
                logger.error(f"ðŸš¨ NEWS STATE TRANSITION VALIDATION FAILED: {state_validation.message}")
            
            # ðŸ” FINAL VALIDATION SUMMARY
            logger.info("ðŸ›¡ï¸ NEWS VALIDATION COMPLETE - All checks performed")
            
            return new_state
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"âŒ NEWS_ANALYST_ULTRA_FAST: Error during analysis: {e}"
            logger.error(error_msg, exc_info=True)
            
            verify_analyst_completion(analyst_name, "error", execution_time, error_msg)
            
            # Return fallback report
            fallback_report = f"""âš ï¸ NEWS ANALYSIS ERROR

ERROR: Failed to fetch current news data for {company}
REASON: {str(e)}
STATUS: Using fallback analysis

RECOMMENDATION: Manual review required for {company} news sentiment.
"""
            
            return {
                "news_report": fallback_report,
                "news_messages": [],
                "sender": "News Analyst (Ultra-Fast - Error)"
            }
    
    return news_analyst_ultra_fast_node


async def gather_news_data(company: str, toolkit, start_time: float) -> Dict[str, Any]:
    """Gather news data from multiple sources with direct API calls"""
    
    news_data = {
        "serper_news": [],
        "finnhub_news": [],
        "total_articles": 0,
        "sources_attempted": 0,
        "sources_successful": 0,
        "data_fetch_time": 0
    }
    
    # Data collection start time
    data_start = time.time()
    
    # Try Serper API for Google News
    try:
        news_data["sources_attempted"] += 1
        logger.info("ðŸ” NEWS_ANALYST_ULTRA_FAST: Fetching Serper news data")
        
        # Direct import of serper utils
        from ..dataflows.serper_utils import getNewsDataSerperAPIWithPagination
        
        # Get API configuration
        config = getattr(toolkit, 'config', {})
        
        # Call Serper API directly
        serper_start = time.time()
        serper_articles = await getNewsDataSerperAPIWithPagination(
            query_or_company=company,
            max_pages=2,  # Limit to 2 pages for speed
            config=config
        )
        serper_time = time.time() - serper_start
        
        if serper_articles:
            news_data["serper_articles"] = serper_articles  # Changed from "serper_news" to match validation schema
            news_data["sources_successful"] += 1
            news_data["total_articles"] += len(serper_articles)
            logger.info(f"âœ… NEWS_ANALYST_ULTRA_FAST: Serper data fetched - {len(serper_articles)} articles in {serper_time:.3f}s")
        else:
            logger.warning("âš ï¸ NEWS_ANALYST_ULTRA_FAST: Serper API returned no results")
            
    except Exception as e:
        logger.error(f"âŒ NEWS_ANALYST_ULTRA_FAST: Serper API error: {e}")
    
    # Try Finnhub API for financial news
    try:
        news_data["sources_attempted"] += 1
        logger.info("ðŸ” NEWS_ANALYST_ULTRA_FAST: Fetching Finnhub news data")
        
        # Use the Finnhub news tool
        if hasattr(toolkit, 'get_finnhub_news'):
            finnhub_start = time.time()
            
            # Calculate date range (last 7 days)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=7)
            
            # Call Finnhub news tool - use invoke method 
            finnhub_result = toolkit.get_finnhub_news.invoke({
                "ticker": company,  
                "start_date": start_date.strftime('%Y-%m-%d'), 
                "end_date": end_date.strftime('%Y-%m-%d')  
            })
            
            finnhub_time = time.time() - finnhub_start
            
            # Parse the result - it's typically a formatted string, we need to extract articles
            if finnhub_result and "No news" not in finnhub_result:
                # Convert the string result to a list format for consistency
                finnhub_articles = parse_finnhub_result(finnhub_result)
                news_data["finnhub_news"] = finnhub_articles
                news_data["sources_successful"] += 1
                news_data["total_articles"] += len(finnhub_articles)
                logger.info(f"âœ… NEWS_ANALYST_ULTRA_FAST: Finnhub data fetched - {len(finnhub_articles)} articles in {finnhub_time:.3f}s")
            else:
                logger.warning("âš ï¸ NEWS_ANALYST_ULTRA_FAST: Finnhub API returned no results")
        else:
            logger.warning("âš ï¸ NEWS_ANALYST_ULTRA_FAST: get_finnhub_news tool not available in toolkit")
                
    except Exception as e:
        logger.error(f"âŒ NEWS_ANALYST_ULTRA_FAST: Finnhub API error: {e}")
    
    # Calculate total data fetch time
    news_data["data_fetch_time"] = time.time() - data_start
    
    logger.info(f"ðŸ“Š NEWS_ANALYST_ULTRA_FAST: Data gathering complete")
    logger.info(f"   ðŸ“° Total articles: {news_data['total_articles']}")
    logger.info(f"   ðŸŽ¯ Sources successful: {news_data['sources_successful']}/{news_data['sources_attempted']}")
    logger.info(f"   â±ï¸ Data fetch time: {news_data['data_fetch_time']:.3f}s")
    
    return news_data


def generate_news_report(company: str, news_data: Dict[str, Any], current_date: str) -> str:
    """Generate structured news analysis report"""
    
    total_articles = news_data["total_articles"]
    serper_count = len(news_data["serper_news"])
    finnhub_count = len(news_data["finnhub_news"])
    
    # Generate timestamp
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Start building the report
    report_lines = [
        f"ðŸ“° NEWS ANALYSIS REPORT - {company}",
        f"Generated: {timestamp}",
        f"Trade Date: {current_date}",
        f"",
        f"## ðŸ“Š DATA SUMMARY",
        f"",
        f"- **Total Articles Analyzed:** {total_articles}",
        f"- **Google News (Serper):** {serper_count} articles",
        f"- **Financial News (Finnhub):** {finnhub_count} articles",
        f"- **Data Sources:** {news_data['sources_successful']}/{news_data['sources_attempted']} successful",
        f"- **Fetch Time:** {news_data['data_fetch_time']:.3f} seconds",
        f""
    ]
    
    if total_articles == 0:
        report_lines.extend([
            "## âš ï¸ NO NEWS DATA AVAILABLE",
            "",
            "**IMPACT:** Unable to assess current news sentiment for trading decision.",
            "**RECOMMENDATION:** Proceed with caution - manual news research recommended.",
            "",
            "**TRADING SIGNAL:** NEUTRAL (Insufficient Data)",
            "**CONFIDENCE:** LOW"
        ])
    else:
        # Analyze sentiment and extract key headlines
        key_headlines = extract_key_headlines(news_data, company)
        sentiment_analysis = analyze_news_sentiment(news_data, company)
        
        report_lines.extend([
            "## ðŸ“ˆ KEY HEADLINES",
            ""
        ])
        
        for i, headline in enumerate(key_headlines[:5], 1):
            source = headline.get('source', 'Unknown')
            title = headline.get('title', 'No title')
            snippet = headline.get('snippet', '')[:100] + "..." if headline.get('snippet') else ''
            
            report_lines.extend([
                f"**{i}. {title}**",
                f"   - Source: {source}",
                f"   - {snippet}" if snippet else "",
                ""
            ])
        
        report_lines.extend([
            "## ðŸŽ¯ SENTIMENT ANALYSIS",
            "",
            f"- **Overall Sentiment:** {sentiment_analysis['overall_sentiment']}",
            f"- **Positive Indicators:** {sentiment_analysis['positive_count']}",
            f"- **Negative Indicators:** {sentiment_analysis['negative_count']}",
            f"- **Neutral/Mixed:** {sentiment_analysis['neutral_count']}",
            "",
            "## ðŸ’¼ TRADING IMPLICATIONS",
            "",
            f"- **News Sentiment Signal:** {sentiment_analysis['trading_signal']}",
            f"- **Confidence Level:** {sentiment_analysis['confidence']}",
            f"- **Risk Assessment:** {sentiment_analysis['risk_level']}",
            "",
            "## ðŸŽ¯ INVESTMENT RECOMMENDATION",
            "",
            f"Based on current news sentiment analysis of {total_articles} articles:",
            f"**SIGNAL: {sentiment_analysis['trading_signal']}**",
            "",
            f"**Rationale:** {sentiment_analysis['rationale']}",
            f"**Risk Level:** {sentiment_analysis['risk_level']}",
            f"**Monitoring:** Continue tracking news sentiment for {company}"
        ])
    
    return "\n".join(report_lines)


def extract_key_headlines(news_data: Dict[str, Any], company: str) -> List[Dict[str, Any]]:
    """Extract and rank key headlines from news data"""
    
    all_headlines = []
    
    # Process Serper news
    for article in news_data["serper_news"]:
        all_headlines.append({
            'title': article.get('title', ''),
            'source': article.get('source', ''),
            'snippet': article.get('snippet', ''),
            'url': article.get('link', ''),
            'date': article.get('date', ''),
            'priority': calculate_headline_priority(article, company),
            'type': 'serper'
        })
    
    # Process Finnhub news
    for article in news_data["finnhub_news"]:
        all_headlines.append({
            'title': article.get('headline', ''),
            'source': article.get('source', 'Finnhub'),
            'snippet': article.get('summary', ''),
            'url': article.get('url', ''),
            'date': datetime.fromtimestamp(article.get('datetime', 0)).strftime('%Y-%m-%d') if article.get('datetime') else '',
            'priority': calculate_headline_priority(article, company),
            'type': 'finnhub'
        })
    
    # Sort by priority (highest first)
    all_headlines.sort(key=lambda x: x['priority'], reverse=True)
    
    return all_headlines


def calculate_headline_priority(article: Dict[str, Any], company: str) -> int:
    """Calculate priority score for headlines (higher = more important)"""
    
    priority = 0
    title = (article.get('title') or article.get('headline', '')).lower()
    snippet = (article.get('snippet') or article.get('summary', '')).lower()
    source = article.get('source', '').lower()
    
    # Company name mention
    if company.lower() in title:
        priority += 10
    if company.lower() in snippet:
        priority += 5
    
    # High-impact keywords
    high_impact_keywords = [
        'earnings', 'revenue', 'profit', 'loss', 'merger', 'acquisition',
        'ceo', 'lawsuit', 'investigation', 'sec', 'fda', 'approval',
        'partnership', 'contract', 'deal', 'breakthrough', 'launch'
    ]
    
    for keyword in high_impact_keywords:
        if keyword in title:
            priority += 8
        if keyword in snippet:
            priority += 4
    
    # Source credibility
    tier1_sources = ['reuters', 'bloomberg', 'wall street journal', 'wsj']
    tier2_sources = ['cnbc', 'marketwatch', 'yahoo finance', 'forbes']
    
    if any(t1 in source for t1 in tier1_sources):
        priority += 6
    elif any(t2 in source for t2 in tier2_sources):
        priority += 3
    
    return priority


def parse_finnhub_result(finnhub_result: str) -> List[Dict[str, Any]]:
    """Parse Finnhub tool result string into article list format"""
    articles = []
    
    try:
        # The finnhub tool returns a formatted string, we need to extract the articles
        # Look for lines that contain article information
        lines = finnhub_result.split('\n')
        current_article = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Look for patterns like "Title: ..." or "Summary: ..."
            if line.startswith('Title:') or line.startswith('Headline:'):
                if current_article:
                    articles.append(current_article)
                    current_article = {}
                current_article['headline'] = line.split(':', 1)[1].strip()
                current_article['title'] = current_article['headline']
            elif line.startswith('Summary:') or line.startswith('Description:'):
                current_article['summary'] = line.split(':', 1)[1].strip()
            elif line.startswith('Source:'):
                current_article['source'] = line.split(':', 1)[1].strip()
            elif line.startswith('Date:') or line.startswith('Published:'):
                current_article['date'] = line.split(':', 1)[1].strip()
            elif line.startswith('URL:') or line.startswith('Link:'):
                current_article['url'] = line.split(':', 1)[1].strip()
        
        # Add the last article if exists
        if current_article:
            articles.append(current_article)
            
        # If no structured parsing worked, create a single article with the full text
        if not articles and finnhub_result.strip():
            articles.append({
                'headline': f'{company} Financial News',
                'title': f'{company} Financial News',
                'summary': finnhub_result.strip()[:500],  # First 500 chars
                'source': 'Finnhub',
                'date': datetime.now().strftime('%Y-%m-%d')
            })
            
    except Exception as e:
        logger.warning(f"âš ï¸ Error parsing Finnhub result: {e}")
        # Fallback: create a single article with the raw result
        if finnhub_result.strip():
            articles.append({
                'headline': f'{company} Financial News',
                'title': f'{company} Financial News', 
                'summary': finnhub_result.strip()[:500],
                'source': 'Finnhub',
                'date': datetime.now().strftime('%Y-%m-%d')
            })
    
    return articles


def analyze_news_sentiment(news_data: Dict[str, Any], company: str) -> Dict[str, Any]:
    """Analyze overall news sentiment and generate trading implications"""
    
    positive_count = 0
    negative_count = 0
    neutral_count = 0
    
    # Sentiment keywords
    positive_keywords = [
        'growth', 'profit', 'revenue', 'success', 'launch', 'expansion',
        'partnership', 'deal', 'approval', 'breakthrough', 'strong',
        'gains', 'beat', 'exceed', 'outperform', 'upgrade', 'buy'
    ]
    
    negative_keywords = [
        'loss', 'decline', 'fall', 'drop', 'concern', 'warning',
        'lawsuit', 'investigation', 'recall', 'failure', 'weak',
        'miss', 'disappoint', 'downgrade', 'sell', 'risk', 'issue'
    ]
    
    # Analyze all articles
    total_articles = 0
    for article in news_data["serper_news"] + news_data["finnhub_news"]:
        total_articles += 1
        
        text_to_analyze = " ".join([
            (article.get('title') or article.get('headline', '')).lower(),
            (article.get('snippet') or article.get('summary', '')).lower()
        ])
        
        # Count sentiment indicators
        pos_score = sum(1 for keyword in positive_keywords if keyword in text_to_analyze)
        neg_score = sum(1 for keyword in negative_keywords if keyword in text_to_analyze)
        
        if pos_score > neg_score:
            positive_count += 1
        elif neg_score > pos_score:
            negative_count += 1
        else:
            neutral_count += 1
    
    # Determine overall sentiment
    if positive_count > negative_count:
        overall_sentiment = "POSITIVE"
        trading_signal = "BUY"
        confidence = "HIGH" if positive_count > negative_count * 1.5 else "MEDIUM"
        risk_level = "LOW"
        rationale = f"Positive news sentiment with {positive_count} positive vs {negative_count} negative indicators"
    elif negative_count > positive_count:
        overall_sentiment = "NEGATIVE"
        trading_signal = "SELL"
        confidence = "HIGH" if negative_count > positive_count * 1.5 else "MEDIUM"
        risk_level = "HIGH"
        rationale = f"Negative news sentiment with {negative_count} negative vs {positive_count} positive indicators"
    else:
        overall_sentiment = "NEUTRAL"
        trading_signal = "HOLD"
        confidence = "MEDIUM"
        risk_level = "MEDIUM"
        rationale = f"Mixed news sentiment with balanced positive/negative indicators"
    
    # Adjust confidence based on data quality
    if total_articles < 5:
        confidence = "LOW"
        rationale += " (Limited news data available)"
    
    return {
        "overall_sentiment": overall_sentiment,
        "positive_count": positive_count,
        "negative_count": negative_count,
        "neutral_count": neutral_count,
        "trading_signal": trading_signal,
        "confidence": confidence,
        "risk_level": risk_level,
        "rationale": rationale
    }